{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Recipe Bot with RAG (with Weaviate Vector DB(locally set) and LangChain Vector Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All steps in a nutshell:\n",
    "\n",
    "Read files | Talk to the database | Load PDFs | Split text | Handle memory | Chat with the model | Create a web UI using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all necessary libraries for RAG bot including Langchain, Weaviate DB, Ollama, and embedding tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Langchain: A framework to build apps using LLMs.\n",
    "\n",
    "* Weaviate: A database that stores text as numbers (vectors) for quick search.\n",
    "\n",
    "* Ollama: A local LLM interface (we‚Äôre using it to talk to the model).\n",
    "\n",
    "* Sentence-transformers: Converts text into vectors (embeddings).\n",
    "\n",
    "* tiktoken: Token counter for OpenAI models (to manage costs and limits)(was mentioned in langchain weaviate doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.17)\n",
      "Requirement already satisfied: langchain_community in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.19)\n",
      "Requirement already satisfied: scikit-learn in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: langchain-ollama in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: sentence-transformers in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: tiktoken in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.65.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: anyio in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_community scikit-learn langchain-ollama sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaviate version has to be 3.26.0 or above getting connection error if not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: weaviate-client\n",
      "Version: 3.26.0\n",
      "Summary: A python native Weaviate client\n",
      "Home-page: https://github.com/weaviate/weaviate-python-client\n",
      "Author: Weaviate\n",
      "Author-email: hello@weaviate.io,\n",
      "License: BSD 3-clause\n",
      "Location: /home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages\n",
      "Requires: authlib, requests, validators\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Weaviate\n",
    "import gradio as gr\n",
    "from typing import Tuple\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What all we are importing:\n",
    "\n",
    "*  Importing weaviate for vector store, \n",
    "\n",
    "*  PyPDFLoader for document loading, \n",
    "\n",
    "*  RecursiveCharacterTextSplitter for text splitting,\n",
    "\n",
    "*  HuggingFaceEmbeddings for embeddings, ChatOllama for conversational AI,\n",
    "\n",
    "*  PromptTemplate for templating, \n",
    "\n",
    "*  ConversationBufferMemory or ConversationVectorMemory for storing conversation history,and ConversationalRetrievalChain for conversational question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load and prepare documents\n",
    "Documents can be anything, we can load a PDF or use webpages as the source also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths to load documents from (the below mentioned book is 102 pages)\n",
    "pdf_paths = [\n",
    "    \"/home/sakhaglobal/Documents/Personal_GitHub/ai-ml-practice/rag-using-llm/recipe-sample.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\.\n",
      "/tmp/ipykernel_60152/1013223438.py:8: DeprecationWarning: invalid escape sequence \\.\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n"
     ]
    }
   ],
   "source": [
    "docs = [PyPDFLoader(pdf_path).load() for pdf_path in pdf_paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# chunk size set to 1000 for better context understanding, overlap set to 200 to avoid missing context\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    add_start_index=True  # Helps track document position\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"/home/sakhaglobal/Documents/Personal_GitHub/chefly/.env\")\n",
    "\n",
    "# Instantiate the Weaviate client for local instance\n",
    "client = weaviate.Client(\n",
    "    url=\"http://localhost:8080\",  # Local Weaviate instance\n",
    ")\n",
    "\n",
    "# Create a class/schema if it doesn't already exist\n",
    "if not client.schema.exists(\"RecipeBot\"):\n",
    "    client.schema.create_class({\n",
    "        \"class\": \"RecipeBot\",\n",
    "        \"vectorizer\": \"none\",  # We're using our own embeddings\n",
    "        \"properties\": [\n",
    "            {\n",
    "                \"name\": \"text\",\n",
    "                \"dataType\": [\"text\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"metadata\",\n",
    "                \"dataType\": [\"text\"],\n",
    "            },\n",
    "            # Add more properties as needed from your metadata\n",
    "            {\n",
    "                \"name\": \"source\",\n",
    "                \"dataType\": [\"text\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page\",\n",
    "                \"dataType\": [\"number\"],  # Changed from text to number for page\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "index_name = \"RecipeBot\"\n",
    "\n",
    "# Print what's being inserted for verification\n",
    "# print(f\"Inserting {len(texts)} documents into Weaviate:\")\n",
    "# print(\"-\" * 50)\n",
    "# for i, (text, metadata) in enumerate(zip(texts, metadatas)):\n",
    "#     print(f\"Document {i+1}:\")\n",
    "#     print(f\"Text snippet: {text[:200]}...\")  # Show first 100 chars\n",
    "#     print(f\"Metadata: {json.dumps(metadata)[:100]}...\")  # Truncate long metadata\n",
    "#     print(f\"Source: {metadata.get('source', 'unknown')}\")\n",
    "#     print(f\"Page: {metadata.get('page', 0)}\")\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = \"/home/sakhaglobal/Documents/Personal_GitHub/chefly/ai/cache\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e5_embed(texts, is_query=False):\n",
    "    prefix = \"query: \" if is_query else \"passage: \"\n",
    "    formatted_texts = [prefix + text.lower().strip() for text in texts]\n",
    "    return embed_model.encode(formatted_texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]\n",
    "embeddings = e5_embed(texts, is_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case CUDA error, if killing the particular PID doesn't work, (as killing PID doesn't work every time for me)\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Free up memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batch.configure(batch_size=50)\n",
    "for i, emb in enumerate(embeddings):\n",
    "    data_obj = {\n",
    "        \"text\": texts[i],\n",
    "        \"metadata\": json.dumps(metadatas[i]),\n",
    "        \"source\": metadatas[i].get(\"source\", \"\"),\n",
    "        \"page\": metadatas[i].get(\"page\", 0)\n",
    "    }\n",
    "    client.batch.add_data_object(\n",
    "        data_object=data_obj,\n",
    "        class_name=index_name,\n",
    "        vector=emb.tolist()\n",
    "        # No UUID specified - Weaviate will generate one\n",
    "    )\n",
    "client.batch.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOllama(model=\"deepseek-r1:14b\")\n",
    "llm = ChatOllama(model=\"mistral-small3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are Recipe Bot, an expert cooking assistant specializing in student-friendly recipes.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Source Knowledge:\n",
    "- Answer ONLY using the recipe book context\n",
    "- Never invent recipes or ingredients\n",
    "- For measurements, be precise (e.g., \"200g mushrooms\")\n",
    "\n",
    "2. Conversation Flow:\n",
    "- Maintain natural, friendly tone\n",
    "- Reference previous answers when appropriate\n",
    "- Acknowledge preferences from chat history\n",
    "- If context is missing, say: \"This isn't covered in my recipe book\"\n",
    "\n",
    "3. Special Cases:\n",
    "- For substitution questions, suggest closest alternatives\n",
    "- For timing questions, specify preparation vs cooking time\n",
    "\n",
    "Examples:\n",
    "Q: Can I substitute X with Y?\n",
    "A: \"Yes, Y works well. Use 25% less as it's more potent.\"\n",
    "\n",
    "Q: How long does this take?\n",
    "A: \"Preparation: 15 mins, Cooking: 30 mins (total 45 mins)\"\n",
    "\n",
    "Q: I don‚Äôt like beef. Are there vegetarian options in the book?\n",
    "A: Yes, the recipe collection includes vegetarian rice and several egg-based dishes like omelette and egg fried rice.\n",
    "\n",
    "Q: Can I make Thai Green Curry easily?\n",
    "A: Yes. Thai green curry is made by cooking curry paste with chicken, onion, and aubergine, then adding coconut milk and simmering until cooked. It‚Äôs a simple and delicious recipe ideal for students.\n",
    "\n",
    "Current Context: {context}\n",
    "Chat History: {chat_history}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document  # Add this import at the top\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "# 1. First create an Embeddings wrapper class for your E5 function\n",
    "class E5EmbeddingsWrapper(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"For documents/passages\"\"\"\n",
    "        return e5_embed(texts, is_query=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"For queries\"\"\"\n",
    "        return e5_embed([text], is_query=True).tolist()[0]\n",
    "\n",
    "# 2. Initialize the embeddings wrapper\n",
    "e5_embeddings = E5EmbeddingsWrapper()\n",
    "\n",
    "# 3. Initialize LangChain Weaviate vector store and retriever\n",
    "# Ensure 'client' (Weaviate client) and 'index_name' (\"RecipeBot\") are defined from previous cells.\n",
    "# Ensure 'e5_embeddings' (E5EmbeddingsWrapper instance) is defined above.\n",
    "vectorstore = Weaviate(\n",
    "    client=client,\n",
    "    index_name=index_name,      # Should be \"RecipeBot\"\n",
    "    text_key=\"text\",            # Key for the main text content in Weaviate\n",
    "    embedding=e5_embeddings,    # Pass the wrapper instance here\n",
    "    attributes=[\"source\", \"page\", \"metadata\"],  # List of metadata fields to retrieve\n",
    "    by_text=False\n",
    ")\n",
    "\n",
    "e5_retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # Retrieve top 3 relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for conversation memory if it doesn't already exist\n",
    "if not client.schema.exists(\"ConversationMemory\"):\n",
    "    client.schema.create_class({\n",
    "        \"class\": \"ConversationMemory\",\n",
    "        \"vectorizer\": \"none\",  # Using custom E5 embeddings\n",
    "        \"properties\": [\n",
    "            {\n",
    "                \"name\": \"text\",\n",
    "                \"dataType\": [\"text\"],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"metadata\",\n",
    "                \"dataType\": [\"text\"], # Storing metadata as a JSON string\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "# 4. Memory-specific vector store (separate from main index)\n",
    "memory_vectorstore = Weaviate(\n",
    "    client=client,\n",
    "    index_name=\"ConversationMemory\",\n",
    "    text_key=\"text\",\n",
    "    attributes=[\"metadata\"],\n",
    "    embedding=e5_embeddings,\n",
    "    by_text=False\n",
    ")\n",
    "\n",
    "memory_retriever = memory_vectorstore.as_retriever(\n",
    "     search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vector memory \n",
    "* better for longer conversations\n",
    "* stores meaning and context necessary for the continuos conversation\n",
    "* has one extra parameter retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = VectorStoreRetrieverMemory(\n",
    "    retriever=memory_retriever,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    "    return_docs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversational chain  > this chain is for conversational memory, so  replacing RetrievalQA with ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=e5_retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},    # same prompt from above\n",
    "    # verbose=True,  # debugging here\n",
    "    rephrase_question=True,  # Helps with follow-up questions\n",
    "    get_chat_history=lambda h: h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "class RAGApplication:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.chat_history = []  # This will store (question, answer) tuples\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        # Convert your chat history to LangChain's expected format\n",
    "        lc_history = []\n",
    "        for q, a in self.chat_history:\n",
    "            lc_history.append(HumanMessage(content=q))\n",
    "            lc_history.append(AIMessage(content=a))\n",
    "\n",
    "        # Call your chain with properly formatted history\n",
    "        result = self.qa_chain({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": lc_history  # Now in correct format\n",
    "        })\n",
    "\n",
    "        # Store the new interaction\n",
    "        self.chat_history.append((question, result[\"answer\"]))\n",
    "        return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your RAG application (use your existing initialization)\n",
    "rag_app = RAGApplication(qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Simple Gradio Chat template for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/gradio/routes.py:1215: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n",
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/gradio/http_server.py:119: ResourceWarning: unclosed <socket.socket fd=109, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/gradio/route_utils.py:871: DeprecationWarning: The loop argument is deprecated since Python 3.8, and scheduled for removal in Python 3.10.\n",
      "  app.stop_event = asyncio.Event(loop=loop)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://2296a9eb71bd15d888.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2296a9eb71bd15d888.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/starlette/templating.py:161: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def chat(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"Handle chat messages\"\"\"\n",
    "    response = rag_app.run(message)\n",
    "    history.append((message, response))\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks(title=\"Recipe Bot\") as demo:\n",
    "    gr.Markdown(\"# üç≥ Recipe Bot\")\n",
    "    gr.Markdown(\"Ask me anything about recipes from docs!\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(label=\"Your question\", placeholder=\"Type your question here...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(\n",
    "        chat,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot]\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
