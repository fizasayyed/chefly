{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Recipe Bot with RAG (with Pinecone Vector DB and LangChain Vector Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful resources:\n",
    "\n",
    "https://www.pinecone.io/learn/retrieval-augmented-generation/#RAG-is-the-most-cost-effective-easy-to-implement-and-lowest-risk-path-to-higher-performance-for-GenAI-applications.\n",
    "\n",
    "https://www.pinecone.io/learn/vector-database/\n",
    "\n",
    "https://docs.pinecone.io/guides/get-started/build-a-rag-chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.17)\n",
      "Requirement already satisfied: langchain_community in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.19)\n",
      "Requirement already satisfied: scikit-learn in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: langchain-ollama in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: sentence-transformers in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: tiktoken in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.65.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: anyio in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_community scikit-learn langchain-ollama sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import List, Dict, Any, Tuple\n",
    "# from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "# from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What all we are importing:\n",
    "\n",
    "*  Importing pinecone for vector store, \n",
    "\n",
    "*  RetrievalQA for question answering,\n",
    "\n",
    "*  PyPDFLoader for document loading, \n",
    "\n",
    "*  RecursiveCharacterTextSplitter for text splitting,\n",
    "\n",
    "*  HuggingFaceEmbeddings for embeddings, ChatOllama for conversational AI,\n",
    "\n",
    "*  PromptTemplate for templating, \n",
    "\n",
    "*  ConversationBufferMemory or ConversationVectorMemory for storing conversation history,and ConversationalRetrievalChain for conversational question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load and prepare documents\n",
    "Documents can be anything, we can load a PDF or use webpages as the source also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths to load documents from (the below mentioned book is 102 pages)\n",
    "pdf_paths = [\n",
    "    \"/home/recipe-sample.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "docs = [PyPDFLoader(pdf_path).load() for pdf_path in pdf_paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# chunk size set to 1000 for better context understanding, overlap set to 200 to avoid missing context\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    add_start_index=True  # Helps track document position\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Using Pinecone as the vector store\n",
    "\n",
    "More details here about why we chose Pinecone: https://docs.google.com/spreadsheets/d/19Usb8_hIsGk61SLxQewUZ4rWexClSa26n1jfoNekCvE/edit?gid=506308235#gid=506308235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating a new instance of the Pinecone client, passing in our API key.\n",
    "\n",
    "We are also defining the name of the index that we will be using for our conversational bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"/home/chefly/.env\")\n",
    "\n",
    "api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "index_name = \"rag-conversational-bot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a multilingual sentence transformer model to convert text into embeddings (vectors of numbers)\n",
    "\n",
    "This specific model is called \"intfloat/multilingual-e5-large\" and it turns text into vectors of 1024 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = \"/home/chefly/ai/cache\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom E5 Embedding Function\n",
    "* This function is used to convert text into embeddings\n",
    "* We use the e5 model to do this, which is a powerful multilingual model\n",
    "* The function takes in a list of text strings and an optional boolean parameter\n",
    "* If the boolean is true, the function will format the strings as queries\n",
    "* Otherwise it will format them as passages\n",
    "* The function then uses the e5 model to encode the formatted strings into embeddings\n",
    "* The embeddings are then normalized and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e5_embed(texts, is_query=False):\n",
    "    prefix = \"query: \" if is_query else \"passage: \"\n",
    "    formatted_texts = [prefix + text.lower().strip() for text in texts]\n",
    "    return embed_model.encode(formatted_texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create E5 embeddings for the documents\n",
    "* The E5 model is a special kind of language model that can be used to generate embeddings for text documents.\n",
    "* These embeddings are a way of representing the text documents as vectors in a high-dimensional space.\n",
    "* The embeddings are then used to search for the most similar documents to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]\n",
    "embeddings = e5_embed(texts, is_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Upsert data with metadata into a Pinecone index.\n",
    "\n",
    "* Upserting because pinecone serverless index automatically creates embeddings for the text inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 19}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [{\n",
    "    \"id\": f\"doc_{i}\",\n",
    "    \"values\": emb.tolist(),\n",
    "    \"metadata\": {**metadatas[i], \"text\": texts[i]}\n",
    "} for i, emb in enumerate(embeddings)]\n",
    "pc.Index(index_name).upsert(vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upsert to Pinecone (optimized batch format)\n",
    "# batch_size = 100\n",
    "# for i in range(0, len(texts), batch_size):\n",
    "#     batch_vectors = [{\n",
    "#         \"id\": f\"doc_{i+j}\",\n",
    "#         \"values\": embeddings[i+j].tolist(),\n",
    "#         \"metadata\": {**metadatas[i+j], \"text\": texts[i+j]}\n",
    "#     } for j in range(min(batch_size, len(texts)-i))]\n",
    "#     index.upsert(vectors=batch_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index if needed\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"mistral-small3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This part of the code is setting up a memory buffer to store the conversation history.(Langchain memory method)\n",
    "* This is used to keep track of the conversation and allow the model to access previous messages.\n",
    "* The memory buffer is set to store a maximum of 2000 tokens, and will trim the oldest messages if this limit is exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True,\n",
    "#     max_token_limit=2000,  # Trims oldest messages if exceeded (NEW)\n",
    "#     input_key = \"question\",\n",
    "#     output_key = \"answer\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using few shot prompting for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "# system_prompt = \"\"\"You are a friendly and practical cooking assistant trained on a collection of student-friendly recipes.\n",
    "# Answer user questions accurately and concisely using the knowledge from this recipe book.\n",
    "# If a question is outside the book‚Äôs scope, politely respond that it's not covered in the source material.\n",
    "\n",
    "# Use these rules:\n",
    "# 1. Answer ONLY from the provided context\n",
    "# 2. For memory questions, use the exact chat history below\n",
    "# 3. If unsure, say \"I don't know\"\n",
    "\n",
    "# Here are some example questions and answers:\n",
    "\n",
    "# Q: I don‚Äôt like beef. Are there vegetarian options in the book?\n",
    "# A: Yes, the recipe collection includes vegetarian rice and several egg-based dishes like omelette and egg fried rice.\n",
    "\n",
    "# Q: Can I make Thai Green Curry easily?\n",
    "# A: Yes. Thai green curry is made by cooking curry paste with chicken, onion, and aubergine, then adding coconut milk and simmering until cooked. It‚Äôs a simple and delicious recipe ideal for students.\n",
    "\n",
    "# Context: {context}\n",
    "# Chat History: {chat_history}\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "#     (\"human\", \"{question}\"),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Prompt 2 : with proper instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are Recipe Bot, an expert cooking assistant specializing in student-friendly recipes.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Source Knowledge:\n",
    "- Answer ONLY using the recipe book context\n",
    "- Never invent recipes or ingredients\n",
    "- For measurements, be precise (e.g., \"200g mushrooms\")\n",
    "\n",
    "2. Conversation Flow:\n",
    "- Maintain natural, friendly tone\n",
    "- Reference previous answers when appropriate\n",
    "- Acknowledge preferences from chat history\n",
    "- If context is missing, say: \"This isn't covered in my recipe book\"\n",
    "\n",
    "3. Special Cases:\n",
    "- For substitution questions, suggest closest alternatives\n",
    "- For timing questions, specify preparation vs cooking time\n",
    "\n",
    "Examples:\n",
    "Q: Can I substitute X with Y?\n",
    "A: \"Yes, Y works well. Use 25% less as it's more potent.\"\n",
    "\n",
    "Q: How long does this take?\n",
    "A: \"Preparation: 15 mins, Cooking: 30 mins (total 45 mins)\"\n",
    "\n",
    "Q: I don‚Äôt like beef. Are there vegetarian options in the book?\n",
    "A: Yes, the recipe collection includes vegetarian rice and several egg-based dishes like omelette and egg fried rice.\n",
    "\n",
    "Q: Can I make Thai Green Curry easily?\n",
    "A: Yes. Thai green curry is made by cooking curry paste with chicken, onion, and aubergine, then adding coconut milk and simmering until cooked. It‚Äôs a simple and delicious recipe ideal for students.\n",
    "\n",
    "Current Context: {context}\n",
    "Chat History: {chat_history}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Create retriever and chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: https://medium.com/@3rdSon/how-to-build-rag-applications-with-pinecone-serverless-openai-langchain-and-python-d4eb263424f1#ca4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26548/2113918543.py:7: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class E5Retriever(BaseRetriever):\n",
      "/tmp/ipykernel_26548/2113918543.py:7: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class E5Retriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document  # Add this import at the top\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "import pinecone\n",
    "\n",
    "class E5Retriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that properly handles Pinecone index\"\"\"\n",
    "\n",
    "    def __init__(self, index: pinecone.Index):\n",
    "        # Bypass Pydantic validation by setting attribute after initialization\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"_index\", index)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_embedding = e5_embed([query], is_query=True).tolist()[0]\n",
    "        results = self._index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=match.metadata[\"text\"],\n",
    "                metadata=match.metadata\n",
    "            ) for match in results.matches\n",
    "        ]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "# Initialize with your existing index\n",
    "e5_retriever = E5Retriever(index=index)  # Your actual Pinecone index\n",
    "\n",
    "\n",
    "# 1. First create an Embeddings wrapper class for your E5 function\n",
    "class E5EmbeddingsWrapper(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"For documents/passages\"\"\"\n",
    "        return e5_embed(texts, is_query=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"For queries\"\"\"\n",
    "        return e5_embed([text], is_query=True).tolist()[0]\n",
    "\n",
    "# 2. Initialize the embeddings wrapper\n",
    "e5_embeddings = E5EmbeddingsWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26548/2518639431.py:3: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  memory_vectorstore = Pinecone(\n"
     ]
    }
   ],
   "source": [
    "# 4. Memory-specific vector store (separate from main index)\n",
    "memory_index = pc.Index(\"conversation-memory\")  # Create separate index\n",
    "memory_vectorstore = Pinecone(\n",
    "    index=memory_index,\n",
    "    embedding=e5_embeddings,\n",
    "    text_key=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vector memory \n",
    "* better for longer conversations\n",
    "* stores meaning and context necessary for the continuos conversation\n",
    "* has one extra parameter retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = VectorStoreRetrieverMemory(\n",
    "    retriever=memory_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    "    return_docs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversational chain  > this chain is for conversational memory, so  replacing RetrievalQA with ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=e5_retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},    # same prompt from above\n",
    "    # verbose=True,  # debugging here\n",
    "    rephrase_question=True,  # Helps with follow-up questions\n",
    "    get_chat_history=lambda h: h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "class RAGApplication:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.chat_history = []  # This will store (question, answer) tuples\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        # Convert your chat history to LangChain's expected format\n",
    "        lc_history = []\n",
    "        for q, a in self.chat_history:\n",
    "            lc_history.append(HumanMessage(content=q))\n",
    "            lc_history.append(AIMessage(content=a))\n",
    "\n",
    "        # Call your chain with properly formatted history\n",
    "        result = self.qa_chain({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": lc_history  # Now in correct format\n",
    "        })\n",
    "\n",
    "        # Store the new interaction\n",
    "        self.chat_history.append((question, result[\"answer\"]))\n",
    "        return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your RAG application (use your existing initialization)\n",
    "rag_app = RAGApplication(qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Simple Gradio Chat template for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://62e4528c03f9bd8f31.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://62e4528c03f9bd8f31.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"Handle chat messages\"\"\"\n",
    "    response = rag_app.run(message)\n",
    "    history.append((message, response))\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks(title=\"Recipe Bot\") as demo:\n",
    "    gr.Markdown(\"# üç≥ Recipe Bot\")\n",
    "    gr.Markdown(\"Ask me anything about recipes from docs!\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(label=\"Your question\", placeholder=\"Type your question here...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(\n",
    "        chat,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot]\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
