{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Recipe Bot with RAG (with Qdrant Vector DB(locally set) and LangChain Vector Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All steps in a nutshell:\n",
    "\n",
    "Read files | Talk to the database | Load PDFs | Split text | Handle memory | Chat with the model | Create a web UI using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all necessary libraries for RAG bot including Langchain, Weaviate DB, Ollama, and embedding tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Langchain: A framework to build apps using LLMs.\n",
    "\n",
    "* Weaviate: A database that stores text as numbers (vectors) for quick search.\n",
    "\n",
    "* Ollama: A local LLM interface (we’re using it to talk to the model).\n",
    "\n",
    "* Sentence-transformers: Converts text into vectors (embeddings).\n",
    "\n",
    "* tiktoken: Token counter for OpenAI models (to manage costs and limits)(was mentioned in langchain weaviate doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.17)\n",
      "Requirement already satisfied: langchain_community in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.2.19)\n",
      "Requirement already satisfied: scikit-learn in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: langchain-ollama in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: sentence-transformers in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: tiktoken in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (0.7.0)\n",
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain-qdrant\n",
      "  Downloading langchain_qdrant-0.1.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (4.65.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from qdrant-client) (1.70.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from qdrant-client) (1.70.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.0)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from qdrant-client) (1.26.20)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.29.5)\n",
      "Requirement already satisfied: setuptools in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (60.2.0)\n",
      "Requirement already satisfied: anyio in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.5.2)\n",
      "Requirement already satisfied: certifi in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: sniffio in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.pyenv/versions/3.8.20/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading qdrant_client-1.12.1-py3-none-any.whl (267 kB)\n",
      "Downloading langchain_qdrant-0.1.4-py3-none-any.whl (23 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: portalocker, hyperframe, hpack, h2, qdrant-client, langchain-qdrant\n",
      "Successfully installed h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 langchain-qdrant-0.1.4 portalocker-2.10.1 qdrant-client-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_community scikit-learn langchain-ollama sentence-transformers tiktoken qdrant-client langchain-qdrant gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaviate version has to be 3.26.0 or above getting connection error if not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: qdrant-client\n",
      "Version: 1.12.1\n",
      "Summary: Client library for the Qdrant vector search engine\n",
      "Home-page: https://github.com/qdrant/qdrant-client\n",
      "Author: Andrey Vasnetsov\n",
      "Author-email: andrey@qdrant.tech\n",
      "License: Apache-2.0\n",
      "Location: /home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages\n",
      "Requires: grpcio, grpcio-tools, httpx, numpy, portalocker, pydantic, urllib3\n",
      "Required-by: langchain-qdrant\n"
     ]
    }
   ],
   "source": [
    "!pip show qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from typing import Tuple\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams # For collection creation (have to learn about this)\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What all we are importing:\n",
    "\n",
    "*  Importing weaviate for vector store, \n",
    "\n",
    "*  PyPDFLoader for document loading, \n",
    "\n",
    "*  RecursiveCharacterTextSplitter for text splitting,\n",
    "\n",
    "*  HuggingFaceEmbeddings for embeddings, ChatOllama for conversational AI,\n",
    "\n",
    "*  PromptTemplate for templating, \n",
    "\n",
    "*  ConversationBufferMemory or ConversationVectorMemory for storing conversation history,and ConversationalRetrievalChain for conversational question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load and prepare documents\n",
    "Documents can be anything, we can load a PDF or use webpages as the source also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths to load documents from (the below mentioned book is 102 pages)\n",
    "pdf_paths = [\n",
    "    \"/home/sakhaglobal/Documents/Personal_GitHub/ai-ml-practice/rag-using-llm/recipe-sample.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Split documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While chunking, we can manually understand what chunk size will fit best to our needs, for me 1000 as the size and 200 as overlap worked good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "docs = [PyPDFLoader(pdf_path).load() for pdf_path in pdf_paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# chunk size set to 1000 for better context understanding, overlap set to 200 to avoid missing context\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    add_start_index=True  # Helps track document position\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using my own embedding model as weaviate local has no own embedding model as cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhaglobal/.pyenv/versions/3.8.20/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = \"/home/sakhaglobal/Documents/Personal_GitHub/chefly/ai/cache\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case CUDA error, if killing the particular PID doesn't work, (as killing PID doesn't work every time for me)\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Free up memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98300/1636862756.py:20: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_service_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'RecipeBot' created/recreated in Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98300/1636862756.py:27: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_service_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'ConversationMemory' created/recreated in Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# This cell should be placed AFTER Cell 15 (embed_model definition)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "# Assumes Qdrant server is running on localhost:6333\n",
    "# For a purely in-memory (non-persistent, no server needed) setup, you could use:\n",
    "# qdrant_service_client = QdrantClient(\":memory:\")\n",
    "qdrant_service_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Get embedding dimension from your E5 model\n",
    "vector_size = embed_model.get_sentence_embedding_dimension() # For intfloat/multilingual-e5-large, this is 1024\n",
    "\n",
    "# Define collection names\n",
    "recipe_collection_name = \"RecipeBot\"\n",
    "memory_collection_name = \"ConversationMemory\"\n",
    "\n",
    "# Recreate \"RecipeBot\" collection (ensures a clean state for reruns)\n",
    "# Use create_collection if you want to avoid deleting existing data and only create if not present\n",
    "qdrant_service_client.recreate_collection(\n",
    "    collection_name=recipe_collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"Collection '{recipe_collection_name}' created/recreated in Qdrant.\")\n",
    "\n",
    "# Recreate \"ConversationMemory\" collection\n",
    "qdrant_service_client.recreate_collection(\n",
    "    collection_name=memory_collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"Collection '{memory_collection_name}' created/recreated in Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e5_embed(texts, is_query=False):\n",
    "    prefix = \"query: \" if is_query else \"passage: \"\n",
    "    formatted_texts = [prefix + text.lower().strip() for text in texts]\n",
    "    return embed_model.encode(formatted_texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOllama(model=\"deepseek-r1:14b\")\n",
    "llm = ChatOllama(model=\"mistral-small3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are Recipe Bot, an expert cooking assistant specializing in student-friendly recipes.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Source Knowledge:\n",
    "- Answer ONLY using the recipe book context\n",
    "- Never invent recipes or ingredients\n",
    "- For measurements, be precise (e.g., \"200g mushrooms\")\n",
    "\n",
    "2. Conversation Flow:\n",
    "- Maintain natural, friendly tone\n",
    "- Reference previous answers when appropriate\n",
    "- Acknowledge preferences from chat history\n",
    "- If context is missing, say: \"This isn't covered in my recipe book\"\n",
    "\n",
    "3. Special Cases:\n",
    "- For substitution questions, suggest closest alternatives\n",
    "- For timing questions, specify preparation vs cooking time\n",
    "\n",
    "Examples:\n",
    "Q: Can I substitute X with Y?\n",
    "A: \"Yes, Y works well. Use 25% less as it's more potent.\"\n",
    "\n",
    "Q: How long does this take?\n",
    "A: \"Preparation: 15 mins, Cooking: 30 mins (total 45 mins)\"\n",
    "\n",
    "Q: I don’t like beef. Are there vegetarian options in the book?\n",
    "A: Yes, the recipe collection includes vegetarian rice and several egg-based dishes like omelette and egg fried rice.\n",
    "\n",
    "Q: Can I make Thai Green Curry easily?\n",
    "A: Yes. Thai green curry is made by cooking curry paste with chicken, onion, and aubergine, then adding coconut milk and simmering until cooked. It’s a simple and delicious recipe ideal for students.\n",
    "\n",
    "Current Context: {context}\n",
    "Chat History: {chat_history}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98300/1190456679.py:22: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.1.2 and will be removed in 0.5.0. Use QdrantVectorStore instead.\n",
      "  vectorstore = Qdrant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 19 documents to Qdrant collection 'RecipeBot'.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document  # Ensure this is imported (likely already is)\n",
    "from langchain.embeddings.base import Embeddings # Ensure this is imported\n",
    "from typing import List # Ensure this is imported\n",
    "\n",
    "# 1. First create an Embeddings wrapper class for your E5 function (this part is unchanged)\n",
    "class E5EmbeddingsWrapper(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"For documents/passages\"\"\"\n",
    "        return e5_embed(texts, is_query=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"For queries\"\"\"\n",
    "        return e5_embed([text], is_query=True).tolist()[0]\n",
    "\n",
    "# 2. Initialize the embeddings wrapper (this part is unchanged)\n",
    "e5_embeddings = E5EmbeddingsWrapper()\n",
    "\n",
    "# 3. Initialize LangChain Qdrant vector store for recipes\n",
    "# qdrant_service_client, recipe_collection_name, and e5_embeddings should be defined from previous cells.\n",
    "# (qdrant_service_client and recipe_collection_name from the new Qdrant setup cell)\n",
    "# (e5_embeddings from just above)\n",
    "vectorstore = Qdrant(\n",
    "    client=qdrant_service_client,\n",
    "    collection_name=recipe_collection_name,\n",
    "    embeddings=e5_embeddings,\n",
    "    # Qdrant Langchain integration maps Document.page_content to a content field\n",
    "    # and Document.metadata to a metadata field in Qdrant payloads automatically.\n",
    "    # The 'attributes' parameter from Weaviate is not used directly; Qdrant stores the full metadata.\n",
    ")\n",
    "\n",
    "# Add documents to the Qdrant collection.\n",
    "# doc_splits (List[Document]) is defined in cell 11.\n",
    "# The E5EmbeddingsWrapper (e5_embeddings) will be used by add_documents internally.\n",
    "# This step replaces the removed cells 17 and 19.\n",
    "vectorstore.add_documents(doc_splits)\n",
    "print(f\"Added {len(doc_splits)} documents to Qdrant collection '{recipe_collection_name}'.\")\n",
    "\n",
    "\n",
    "e5_retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # Retrieve top 3 relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Weaviate-specific schema creation for \"ConversationMemory\" is removed.\n",
    "# The new Qdrant setup cell already created the \"ConversationMemory\" collection.\n",
    "\n",
    "# memory_collection_name, qdrant_service_client, and e5_embeddings should be defined.\n",
    "memory_vectorstore = Qdrant(\n",
    "    client=qdrant_service_client,\n",
    "    collection_name=memory_collection_name,\n",
    "    embeddings=e5_embeddings,\n",
    "    # Langchain Qdrant integration handles metadata and content keys.\n",
    "    # The 'text_key' and 'attributes' from Weaviate are not directly analogous.\n",
    "    # VectorStoreRetrieverMemory will store Langchain Document objects,\n",
    "    # and Qdrant will store their page_content and metadata.\n",
    ")\n",
    "\n",
    "memory_retriever = memory_vectorstore.as_retriever(\n",
    "     search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vector memory \n",
    "* better for longer conversations\n",
    "* stores meaning and context necessary for the continuos conversation\n",
    "* has one extra parameter retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = VectorStoreRetrieverMemory(\n",
    "    retriever=memory_retriever,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    "    return_docs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversational chain  > this chain is for conversational memory, so  replacing RetrievalQA with ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=e5_retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},    # same prompt from above\n",
    "    # verbose=True,  # debugging here\n",
    "    rephrase_question=True,  # Helps with follow-up questions\n",
    "    get_chat_history=lambda h: h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "class RAGApplication:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.chat_history = []  # This will store (question, answer) tuples\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        # Convert your chat history to LangChain's expected format\n",
    "        lc_history = []\n",
    "        for q, a in self.chat_history:\n",
    "            lc_history.append(HumanMessage(content=q))\n",
    "            lc_history.append(AIMessage(content=a))\n",
    "\n",
    "        # Call your chain with properly formatted history\n",
    "        result = self.qa_chain({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": lc_history  # Now in correct format\n",
    "        })\n",
    "\n",
    "        # Store the new interaction\n",
    "        self.chat_history.append((question, result[\"answer\"]))\n",
    "        return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your RAG application (use your existing initialization)\n",
    "rag_app = RAGApplication(qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Simple Gradio Chat template for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://f532c58c16ff041861.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f532c58c16ff041861.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98300/1024074564.py:16: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = self.qa_chain({\n"
     ]
    }
   ],
   "source": [
    "def chat(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"Handle chat messages\"\"\"\n",
    "    response = rag_app.run(message)\n",
    "    history.append((message, response))\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks(title=\"Recipe Bot\") as demo:\n",
    "    gr.Markdown(\"# 🍳 Recipe Bot\")\n",
    "    gr.Markdown(\"Ask me anything about recipes from docs!\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(label=\"Your question\", placeholder=\"Type your question here...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(\n",
    "        chat,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot]\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
